{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dlk8qe4ZOgf1drFBdDV-5bBAVZSo4sNf",
      "authorship_tag": "ABX9TyM/C73Ex3S6POydtbBcBAOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ale-chen/Arbitrage/blob/main/FormatData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Goal:** *Predict Arbitrage Profit Margin for a Given Premier League Game*\n",
        "\n",
        "---\n",
        "**X:** (What we are training on)\n",
        "- Data from Previous *5* games involving either team (total 10 games)\n",
        "- Current Game Date, Time, Team IDs\n",
        "\n",
        "**y:** (What we are predicting)\n",
        "- Possible Profit Margin: Categorized into 1/4 classes, based on percent margin"
      ],
      "metadata": {
        "id": "m2L78CNUIP2h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82mzPvjOHhrW"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import os\n",
        "import glob\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"drive/MyDrive/prem_data/data\")"
      ],
      "metadata": {
        "id": "ovyXp92xIGiy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = glob.glob(\"*.csv\")"
      ],
      "metadata": {
        "id": "-Ddm-JIjMT1D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for file in files:\n",
        "  try:\n",
        "    dfs.append(pd.read_csv(file, parse_dates = [\"Date\"], dayfirst = True))\n",
        "  except:\n",
        "    print(\"Error intaking \" + str(file))\n",
        "    continue"
      ],
      "metadata": {
        "id": "6ifVpEegMxs-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check all columns align\n",
        "if all([set(dfs[0].columns) == set(df.columns) for df in dfs]):\n",
        "    print('All have the same columns')\n",
        "else:\n",
        "    print('Some have different columns')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv-MHFixOB7G",
        "outputId": "5b8c022a-c515-433d-a4f6-d31d398f34a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some have different columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keep Only Match Stats, Market (Aggregate) Odds\n",
        "\n",
        "odds_cols = [\"B365%s\",\"BS%s\",\"BW%s\",\"GB%s\",\"IW%s\",\"LB%s\",\"PS%s\",'P%s','SO%s','SB%s','SJ%s','SY%s','VC%s','WH%s']\n",
        "\n",
        "home_odds_cols = [str(i) % \"H\" for i in odds_cols]\n",
        "draw_odds_cols = [str(i) % \"D\" for i in odds_cols]\n",
        "away_odds_cols = [str(i) % \"A\" for i in odds_cols]"
      ],
      "metadata": {
        "id": "dht2g_rOOWCq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfsSlim = []\n",
        "\n",
        "for df in dfs:\n",
        "  dfsSlim.append(df.loc[:, [\n",
        "      'Date','HomeTeam','AwayTeam','FTHG','FTAG','FTR','HTHG','HTAG',\n",
        "      'HTR','HS','AS','HST','AST','HF','AF','HC','AC','HY','AY','HR','AR'\n",
        "      ]])"
      ],
      "metadata": {
        "id": "wfIORxitXDE1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For each row, each df get Max Odds for all 3 outcomes\n",
        "\n",
        "for i, df in enumerate(dfs):\n",
        "  maxH = []\n",
        "  maxD = []\n",
        "  maxA = []\n",
        "  for index, row in df.iterrows():\n",
        "    oddsH = []\n",
        "    oddsD = []\n",
        "    oddsA = []\n",
        "    for colOdd in home_odds_cols:\n",
        "      try:\n",
        "        oddsH.append(row[colOdd])\n",
        "      except:\n",
        "        continue\n",
        "    for colOdd in draw_odds_cols:\n",
        "      try:\n",
        "        oddsD.append(row[colOdd])\n",
        "      except:\n",
        "        continue\n",
        "    for colOdd in away_odds_cols:\n",
        "      try:\n",
        "        oddsA.append(row[colOdd])\n",
        "      except:\n",
        "        continue\n",
        "    maxH.append(max(oddsH))\n",
        "    maxD.append(max(oddsD))\n",
        "    maxA.append(max(oddsA))\n",
        "  \n",
        "  dfsSlim[i]['MAXH'] = maxH\n",
        "  dfsSlim[i]['MAXD'] = maxD\n",
        "  dfsSlim[i]['MAXA'] = maxA"
      ],
      "metadata": {
        "id": "IV65oIpXYd6H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combine all dfs\n",
        "premier = pd.concat(dfsSlim)\n",
        "premier['Date'] = pd.to_datetime(premier['Date']) #Reshuffle by date\n",
        "premier = premier.sort_values(by='Date', ignore_index = True)\n",
        "\n",
        "#Drop the single(?) na row\n",
        "premier.loc[pd.isna(premier[\"MAXH\"]), :].index\n",
        "premier = premier[:-1]\n",
        "\n",
        "teams = {}\n",
        "\n",
        "for team in premier[\"HomeTeam\"]: #Create dictionary of teams \n",
        "  if team not in teams.values():\n",
        "    teams[len(teams)] = team\n",
        "for team in premier[\"AwayTeam\"]:\n",
        "  if team not in teams.values():\n",
        "    teams[len(teams)] = team\n",
        "\n",
        "#teams but keys and vals flipped\n",
        "teamsInv = dict([(value, key) for key, value in teams.items()])\n",
        "\n",
        "premier[\"HomeTeam\"] = premier[\"HomeTeam\"].apply(lambda x: teamsInv[x])\n",
        "premier[\"AwayTeam\"] = premier[\"AwayTeam\"].apply(lambda x: teamsInv[x])\n",
        "\n",
        "outcome = {\n",
        "    \"H\": -1,\n",
        "    \"D\": 0,\n",
        "    \"A\": 1\n",
        "}\n",
        "#for FTR and HTR, replace H/D/A with -1,0,1\n",
        "premier[\"FTR\"] = premier[\"FTR\"].apply(lambda x: outcome[x])\n",
        "premier[\"HTR\"] = premier[\"HTR\"].apply(lambda x: outcome[x])\n",
        "\n",
        "#Reset Date to Ordinal to use as input data\n",
        "premier[\"Date\"] = premier[\"Date\"].apply(lambda x: pd.Timestamp.toordinal(x))"
      ],
      "metadata": {
        "id": "McQk-rhjdrky"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "#GENERATE ONE ROW OF INPUT DATA\n",
        "def gen_X(df: pd.DataFrame, index: int): #WILL THROW ERROR IF THERE ARE TOO FEW GAMES BEFORE INDEX\n",
        "  hometeam = df['HomeTeam'][index]\n",
        "  awayteam = df['AwayTeam'][index]\n",
        "\n",
        "  homegames = []\n",
        "  awaygames = []\n",
        "\n",
        "  search = df.truncate(after=index)\n",
        "#REVERSE ORDER IN ORDER TO SEARCH FROM CLOSEST TO FARTHEST\n",
        "  search = search.loc[::-1]\n",
        "\n",
        "#WILL THROW ERROR IF THERE ARE TOO FEW GAMES BEFORE INDEX\n",
        "#REMEMBER TO CATCH LATER\n",
        "\n",
        "  for i in range(len(search)):\n",
        "    if(len(homegames) < 5):\n",
        "      if df.iloc[i]['HomeTeam'] == hometeam or df.iloc[i]['AwayTeam'] == hometeam:\n",
        "        homegames.append(df.iloc[[i]])\n",
        "    if(len(awaygames) < 5):\n",
        "      if df.iloc[i]['HomeTeam'] == awayteam or df.iloc[i]['AwayTeam'] == awayteam:\n",
        "        awaygames.append(df.iloc[[i]])\n",
        "    if(len(homegames) == 5 and len(awaygames) == 5):\n",
        "      break\n",
        "  \"\"\"\n",
        "  for i in search.iterrows():\n",
        "    print(i)\n",
        "    if(len(homegames) == 5 and len(homegames) == 5):\n",
        "      break\n",
        "    else:\n",
        "      if i[1][1] == hometeam or i[1][2] == hometeam: #i[1][1] is home team\n",
        "        homegames.append(i)\n",
        "      if i[1][1] == awayteam or i[1][2] == awayteam: #i[1][2] is away team\n",
        "        awaygames.append(i)\n",
        "  \"\"\"\n",
        "#PREDICTOR CANNOT HAVE ACCESS TO CURRENT BETTING DATA FOR PREDICTION\n",
        "  result = df.iloc[[index]].drop(columns = ['MAXH','MAXD','MAXA'])\n",
        "  \n",
        "  for index, game in enumerate(homegames):\n",
        "    game.rename(columns = lambda s: s + \"_H_\" + str(index), inplace = True)\n",
        "    result = pd.concat(\n",
        "        [result.reset_index(drop=True),\n",
        "         game.reset_index(drop=True)],\n",
        "         axis = 1)\n",
        "  \n",
        "  for game in awaygames:\n",
        "    game.rename(columns = lambda s: s + \"_A_\" + str(index), inplace = True)\n",
        "    result = pd.concat(\n",
        "        [result.reset_index(drop=True),\n",
        "         game.reset_index(drop=True)],\n",
        "         axis = 1)\n",
        "  \n",
        "  #print(tabulate(result, headers = 'keys', tablefmt = 'psql'))\n",
        "  \n",
        "  #Returns result, number of historical games as tuple\n",
        "  return result, len(homegames) + len(awaygames)"
      ],
      "metadata": {
        "id": "2u5H8idPexF8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GENERATE ONE ROW OF TARGET DATA (Basically output arb level)\n",
        "#We want to turn a regression problem into a classification problem\n",
        "\"\"\"\n",
        "idx | profit range\n",
        "-------------------\n",
        "-2:   [-inf, -2%]\n",
        "-1:   [-2%, -0%]\n",
        "1 :   [+0%, +2%]\n",
        "2 :   [+2%, +inf]\n",
        "\"\"\"\n",
        "def gen_y(df: pd.DataFrame, index: int):\n",
        "  maxH = df.iloc[index][\"MAXH\"]\n",
        "  maxD = df.iloc[index][\"MAXD\"]\n",
        "  maxA = df.iloc[index][\"MAXA\"]\n",
        "  \n",
        "  profit = arbProfit(maxH, maxD, maxA)\n",
        "# UNCOMMENT THIS CODE BLOCK TO RETURN CATEGORICAL DATA\n",
        "  \"\"\"\n",
        "  if profit < -.02:\n",
        "    result = -2\n",
        "  elif -.02 <= profit < 0:\n",
        "    result = -1\n",
        "  elif 0 <= profit < .02:\n",
        "    result = 1\n",
        "  else:\n",
        "    result = 2\n",
        "  \"\"\"\n",
        "# COMMENT OUT THIS CODE BLOCK TO RETURN CATEGORICAL DATA\n",
        "  result = profit\n",
        "\n",
        "  return result\n",
        "\n",
        "#Intakes Odds, returns unbiased profit margin\n",
        "def arbProfit(H,D,A):\n",
        "  impliedWin = (1/H) + (1/D) + (1/A)\n",
        "  return (1/impliedWin) - 1 \n",
        "\n",
        "#For fun, returns bet amounts to result in arbProfit outcome\n",
        "def betAmt(investment, H,D,A):\n",
        "  impliedWin = (1/H) + (1/D) + (1/A)\n",
        "  hAmt = (investment * (1/H)) / impliedWin\n",
        "  dAmt = (investment * (1/D)) / impliedWin\n",
        "  aAmt = (investment * (1/A)) / impliedWin\n",
        "\n",
        "  return hAmt, dAmt, aAmt"
      ],
      "metadata": {
        "id": "8AosAJaDNabf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "\"\"\"\n",
        "PROCESS ALL DATA, ONLY APPEND DATA WHICH HAS 10 PREVIOUS GAMES\n",
        "EXPORT TO CSV\n",
        "THIS TOOK *30 MINUTES* TO COMPLETE\n",
        "\"\"\"\n",
        "\n",
        "for i in range(len(premier)):\n",
        "  if i % 100 == 0:\n",
        "    print(\"Processed next 100 rows; on row #\" + str(i))\n",
        "  Xline, trainGames = gen_X(premier, i)\n",
        "  if(trainGames == 10):\n",
        "    X.append(Xline.loc[0, :].values.flatten().tolist())\n",
        "    y.append(gen_y(premier, i))\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(\"X.csv\",\"w+\") as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "    csvWriter.writerows(X)\n",
        "with open(\"y.csv\",\"w+\") as my_csv:\n",
        "    csvWriter = csv.writer(my_csv,delimiter=',')\n",
        "    csvWriter.writerow(y)\n",
        "\n",
        "\"\"\"\n",
        "y = []\n",
        "for i in range(len(premier)):\n",
        "  y.append(gen_y(premier, i))\n",
        "unique, counts = np.unique(y, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "print(tabulate(premier.iloc[[1210]], headers = 'keys', tablefmt = 'psql'))\n",
        "len(premier)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FfsGDlC08qAw",
        "outputId": "aed73e88-956a-4f07-b78c-392428975d81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed next 100 rows; on row #0\n",
            "Processed next 100 rows; on row #100\n",
            "Processed next 100 rows; on row #200\n",
            "Processed next 100 rows; on row #300\n",
            "Processed next 100 rows; on row #400\n",
            "Processed next 100 rows; on row #500\n",
            "Processed next 100 rows; on row #600\n",
            "Processed next 100 rows; on row #700\n",
            "Processed next 100 rows; on row #800\n",
            "Processed next 100 rows; on row #900\n",
            "Processed next 100 rows; on row #1000\n",
            "Processed next 100 rows; on row #1100\n",
            "Processed next 100 rows; on row #1200\n",
            "Processed next 100 rows; on row #1300\n",
            "Processed next 100 rows; on row #1400\n",
            "Processed next 100 rows; on row #1500\n",
            "Processed next 100 rows; on row #1600\n",
            "Processed next 100 rows; on row #1700\n",
            "Processed next 100 rows; on row #1800\n",
            "Processed next 100 rows; on row #1900\n",
            "Processed next 100 rows; on row #2000\n",
            "Processed next 100 rows; on row #2100\n",
            "Processed next 100 rows; on row #2200\n",
            "Processed next 100 rows; on row #2300\n",
            "Processed next 100 rows; on row #2400\n",
            "Processed next 100 rows; on row #2500\n",
            "Processed next 100 rows; on row #2600\n",
            "Processed next 100 rows; on row #2700\n",
            "Processed next 100 rows; on row #2800\n",
            "Processed next 100 rows; on row #2900\n",
            "Processed next 100 rows; on row #3000\n",
            "Processed next 100 rows; on row #3100\n",
            "Processed next 100 rows; on row #3200\n",
            "Processed next 100 rows; on row #3300\n",
            "Processed next 100 rows; on row #3400\n",
            "Processed next 100 rows; on row #3500\n",
            "Processed next 100 rows; on row #3600\n",
            "Processed next 100 rows; on row #3700\n",
            "Processed next 100 rows; on row #3800\n",
            "Processed next 100 rows; on row #3900\n",
            "Processed next 100 rows; on row #4000\n",
            "Processed next 100 rows; on row #4100\n",
            "Processed next 100 rows; on row #4200\n",
            "Processed next 100 rows; on row #4300\n",
            "Processed next 100 rows; on row #4400\n",
            "Processed next 100 rows; on row #4500\n",
            "Processed next 100 rows; on row #4600\n",
            "Processed next 100 rows; on row #4700\n",
            "Processed next 100 rows; on row #4800\n",
            "Processed next 100 rows; on row #4900\n",
            "Processed next 100 rows; on row #5000\n",
            "Processed next 100 rows; on row #5100\n",
            "Processed next 100 rows; on row #5200\n",
            "Processed next 100 rows; on row #5300\n",
            "Processed next 100 rows; on row #5400\n",
            "Processed next 100 rows; on row #5500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ny = []\\nfor i in range(len(premier)):\\n  y.append(gen_y(premier, i))\\nunique, counts = np.unique(y, return_counts=True)\\nprint(dict(zip(unique, counts)))\\n\\nprint(tabulate(premier.iloc[[1210]], headers = 'keys', tablefmt = 'psql'))\\nlen(premier)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "premier.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meaxU3QS3zHM",
        "outputId": "8947fd61-f41c-4b5a-c18b-3f1c12d8c761"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG',\n",
              "       'HTR', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC', 'HY', 'AY',\n",
              "       'HR', 'AR', 'MAXH', 'MAXD', 'MAXA'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}